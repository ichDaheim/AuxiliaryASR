log_dir: "Checkpoint"
save_freq: 1
save_every_steps: 10
device: "cpu"
epochs: 200
batch_size: 8 # Kleinere Batch-Größe, die sicher in den Speicher passt
gradient_accumulation_steps: 4 # 8 * 4 = 32 effektive Batch-Größe
pretrained_model: "Checkpoint/step_000000710.pth"
load_only_params: false
train_data: "Data/train_list.txt"
val_data: "Data/val_list.txt"

preprocess_parasm:
  sr: 24000
  spect_params:
    n_fft: 2048
    win_length: 1200
    hop_length: 300
  mel_params:
    n_mels: 80

model_params:
   input_dim: 80
   hidden_dim: 256
   n_token: 62
   token_embedding_dim: 256

optimizer_params:
  lr: 0.0005
